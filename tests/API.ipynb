{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyod\n",
    "from typing import Any, Literal, Protocol, Self, Optional, overload\n",
    "import numpy as np\n",
    "import numpy.typing as npt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# relevant PyOD api distilled down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = npt.NDArray[np.float_]  # (n_samples, n_features)\n",
    "SCORES = npt.NDArray[np.float_]  # (n_samples,) higher = more anomalous\n",
    "LABELS = npt.NDArray[np.int_]  # (n_samples,) 0 for inliers, (>=)1 for outliers\n",
    "\n",
    "\n",
    "class PyODModel(Protocol):\n",
    "    decision_scores_: DATA  # train scores\n",
    "    labels_: LABELS  # train predicted labels\n",
    "    threshold_: float\n",
    "\n",
    "    def __init__(self, contamination: float, **kwargs):\n",
    "        ...\n",
    "\n",
    "    def fit(self, X: DATA, y: Optional[LABELS]) -> Self:\n",
    "        \"\"\"Fit detector. y is ignored in unsupervised methods.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def decision_function(self, X: DATA) -> SCORES:\n",
    "        \"\"\"Predict raw anomaly scores of X using the fitted detector.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def predict(\n",
    "        self, X: DATA, return_confidence: bool\n",
    "    ) -> LABELS | tuple[LABELS, SCORES]:\n",
    "        \"\"\"Predict if a particular sample is an outlier or not.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def predict_proba(\n",
    "        self, X: DATA, method: str, return_confidence: bool\n",
    "    ) -> SCORES | tuple[SCORES, SCORES]:\n",
    "        \"\"\"Predict the probability of a sample being outlier.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def predict_confidence(self, X: DATA) -> SCORES:\n",
    "        \"\"\"Predict the model's confidence in making the same prediction\n",
    "        under slightly different training sets.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def _predict_rank(self, X: DATA, normalized: bool) -> SCORES:\n",
    "        \"\"\"Predict the outlyingness rank of a sample by a fitted model.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def _set_n_classes(self, y: LABELS) -> Self:\n",
    "        \"\"\"Set the number of classes if `y` is presented, which is not\n",
    "        expected. It could be useful for multi-class outlier detection.\"\"\"\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal requirements to apply balif-like framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = npt.NDArray[np.int_]  # (n_samples,)\n",
    "\n",
    "class BaeysianableDetector:\n",
    "    def apply(self, X: DATA)->npt.NDArray[np.int_] :\n",
    "        \"\"\"Finds the terminal region idx for each sample in X.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def predict_proba(\n",
    "        self, X: DATA, return_confidence: bool\n",
    "    ) -> SCORES | tuple[SCORES, SCORES]:\n",
    "        ...\n",
    "\n",
    "class BaeysianableEnsemble:\n",
    "    @property\n",
    "    def estimators_(self) -> list[BaeysianableDetector]:\n",
    "        ...\n",
    "\n",
    "BETADISTR = tuple[npt.NDArray[np.float_], npt.NDArray[np.float_]]\n",
    "\n",
    "class BaeysifiedDetector(BaeysianableDetector):\n",
    "    def predict_distr(self, X: DATA) -> BETADISTR:\n",
    "        ...\n",
    "\n",
    "    def update(self, X: DATA, y: LABELS) -> Self:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending PYOD IF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example of IF usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.utils.data import generate_data\n",
    "from pyod.utils.example import visualize\n",
    "from pyod.models.iforest import IForest\n",
    "\n",
    "X_train, X_test, y_train, y_test = generate_data(\n",
    "    n_train=1000, n_test=100, contamination=0.1, random_state=0\n",
    ")\n",
    "X = X_test.astype(np.float32)\n",
    "\n",
    "model = IForest().fit(X_train)\n",
    "y_train_pred, y_test_pred = model.predict(X_train), model.predict(X_test)\n",
    "\n",
    "visualize(\"IF\", X_train, y_train, X_test, y_test, y_train_pred, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant API distilled down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.iforest import IForest\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn.tree._tree import Tree\n",
    "\n",
    "class IForest(PyODModel):\n",
    "    @property\n",
    "    def detector_(self)->IsolationForest:\n",
    "        \"\"\"The underlying sklearn IsolationForest object.\"\"\"\n",
    "        ...\n",
    "\n",
    "    @property\n",
    "    def estimators_(self)->list[ExtraTreeRegressor]:\n",
    "        \"\"\"The collection of fitted sub-estimators.\"\"\"\n",
    "        ...\n",
    "\n",
    "    @property\n",
    "    def estimators_samples_(self)->list[npt.NDArray[np.int_]]:\n",
    "        \"\"\"The subset of in-bag samples for each base estimator.\"\"\"\n",
    "        ...\n",
    "\n",
    "    @property\n",
    "    def max_samples_(self):\n",
    "        \"\"\"The actual number of samples.\"\"\"\n",
    "        ...\n",
    "\n",
    "# sklearn classes follow the API:\n",
    "\n",
    "class IsolationForest(Protocol):\n",
    "    @property\n",
    "    def tree_(self)->Tree:\n",
    "        ... \n",
    "\n",
    "class ExtraTreeRegressor(Protocol):\n",
    "    @property\n",
    "    def tree_(self)->Tree:\n",
    "        ... \n",
    "\n",
    "class Tree:\n",
    "    #THIS ARE ALL PROPERTIES, NOT CLASS ATTRIBUTES\n",
    "\n",
    "    # basic tree structure\n",
    "    max_depth: int\n",
    "    node_count: int\n",
    "    n_leaves: int\n",
    "    children_left: npt.NDArray[np.int_] # (node_count,)\n",
    "    children_right: npt.NDArray[np.int_] # (node_count,)\n",
    "    \n",
    "    # tree fitting information\n",
    "    feature: npt.NDArray[np.int_] # (node_count,)\n",
    "    threshold: npt.NDArray[np.float_] # (node_count,)\n",
    "    n_node_samples: npt.NDArray[np.int_] # (node_count,)\n",
    "\n",
    "    # prediction\n",
    "    value: npt.NDArray[np.float_] # (node_count, n_outputs, max_n_classes)\n",
    "\n",
    "    def compute_node_depths(self)->npt.NDArray[np.int_]:\n",
    "        \"\"\"Compute the depth of each node.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def apply(self, X: DATA)->npt.NDArray[np.int_] :\n",
    "        \"\"\"Finds the terminal region (=leaf node) for each sample in X.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def decision_path(self, X: DATA)->npt.NDArray[np.int_]:\n",
    "        \"\"\"Returns the decision path in the tree.\n",
    "        this is going to be a sparse matrix with shape (n_samples, n_nodes)\")\n",
    "        1 means the node is in the path, 0 means it is not\n",
    "        NOTE: not returned as scipy sparse int matrix\n",
    "            convert before using it with .toarray().astype(bool)\"\"\"\n",
    "        ...\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"you can access the underlying sklearn IsolationForest object:\")\n",
    "sklearn_iforest = model.detector_\n",
    "print(\"model.detector_ is a\", type(sklearn_iforest))\n",
    "print()\n",
    "\n",
    "print(\"this object has some usefull precomputed values:\")\n",
    "c, *_ = model.detector_._average_path_length_per_tree\n",
    "path_lenght, *_ = model.detector_._decision_path_lengths\n",
    "print(f\" - average_path_length: {type(c)}({c.shape}, {c.dtype})\")\n",
    "print(f\" - path_lengths: {type(path_lenght)}({path_lenght.shape}, {path_lenght.dtype})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"you can access individual ExtraRegressionTree:\")\n",
    "estimator, *_ = model.estimators_\n",
    "print(\"model.estimators_ is a\", type(model.estimators_), \"of\", type(estimator))\n",
    "print()\n",
    "\n",
    "print(\"from each estimator you can access the undelying tree:\")\n",
    "tree = estimator.tree_\n",
    "print(\"a tree is a\", tree)\n",
    "print()\n",
    "\n",
    "print(\"with this tree you can:\")\n",
    "leaves = tree.apply(X)\n",
    "leaves = estimator.apply(X)\n",
    "print(f\" - apply(X): get the leaf nodes for X\")\n",
    "print(f\"             ({type(leaves)}({leaves.shape}, {leaves.dtype})\") \n",
    "print()   \n",
    "\n",
    "path = tree.decision_path(X)\n",
    "path = estimator.decision_path(X)\n",
    "print(f\" - decision_path(X): get the decision path for X\")\n",
    "print(f\"                     this is going to be a sparse matrix with shape (n_samples, n_nodes)\")\n",
    "print(f\"                     1 means the node is in the path, 0 means it is not\")\n",
    "print(f\"                     ({type(path)}({path.shape}, {path.dtype})\")\n",
    "print()\n",
    "\n",
    "depths = tree.compute_node_depths()\n",
    "print(f\" - compute_node_depths(): get the depth of each node\")\n",
    "print(f\"                          ({type(depths)}({depths.shape}, {depths.dtype})\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extenting the ExtraTreeRegressor with bayes info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = model.detector_._average_path_length_per_tree\n",
    "path_lenght = model.detector_._decision_path_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from joblib import Parallel\n",
    "# from joblib.parallel import delayed\n",
    "# from sklearn.ensemble import IsolationForest\n",
    "# from sklearn.utils import check_array\n",
    "# from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pyod.models.base import BaseDetector\n",
    "from pyod.models.iforest import IForest\n",
    "from scipy.stats import beta\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BetaDistr():\n",
    "    a: npt.NDArray[np.float_]\n",
    "    b: npt.NDArray[np.float_]\n",
    "\n",
    "    def mean(self):\n",
    "        return beta.mean(self.a, self.b)\n",
    "    \n",
    "    def var(self):\n",
    "        return beta.var(self.a, self.b)\n",
    "\n",
    "    def nu(self):\n",
    "        return self.a + self.b\n",
    "\n",
    "    def update(self, y: LABELS): \n",
    "        self.a += np.sum(y >= 1)\n",
    "        self.b += np.sum(y == 0)   \n",
    "\n",
    "\n",
    "class Wrapper\n",
    "\n",
    "\n",
    "\n",
    "class Balif(IForest):   \n",
    "    threshold_: float\n",
    "\n",
    "    def __init__(self, *args, prior_strength=0.1, score_to_prob_method=\"linear\", **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.prior_strength = prior_strength\n",
    "        self.score_to_prob_method = score_to_prob_method\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        super().fit(X, y)\n",
    "\n",
    "        self.\n",
    "        #TODO\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        #TODO\n",
    "\n",
    "\n",
    "    # def predict(\n",
    "    #     self, X: DATA, return_confidence: bool\n",
    "    # ) -> LABELS | tuple[LABELS, SCORES]:\n",
    "    #     \"\"\"Predict if a particular sample is an outlier or not.\"\"\"\n",
    "    #     ...\n",
    "\n",
    "    # def predict_proba(\n",
    "    #     self, X: DATA, method: str, return_confidence: bool\n",
    "    # ) -> SCORES | tuple[SCORES, SCORES]:\n",
    "    #     \"\"\"Predict the probability of a sample being outlier.\"\"\"\n",
    "    #     ...\n",
    "\n",
    "    # def predict_confidence(self, X: DATA) -> SCORES:\n",
    "    #     \"\"\"Predict the model's confidence in making the same prediction\n",
    "    #     under slightly different training sets.\"\"\"\n",
    "    #     ...\n",
    "\n",
    "    # def _predict_rank(self, X: DATA, normalized: bool) -> SCORES:\n",
    "    #     \"\"\"Predict the outlyingness rank of a sample by a fitted model.\"\"\"\n",
    "    #     ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.iforest import IForest\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn.tree._tree import Tree\n",
    "\n",
    "class BayesianExtraTreeRegressor(ExtraTreeRegressor):\n",
    "    @classmethod\n",
    "    def wrap(cls, regressor:ExtraTreeRegressor)->Self:\n",
    "        self.regressor = regressor\n",
    "        self.alpha, self.beta = cls.compute_prior(regressor)\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_prior(regressor:ExtraTreeRegressor)->BETADISTR:\n",
    "        ...\n",
    "\n",
    "    def predict_distr(self, X: DATA) -> BETADISTR:\n",
    "        ...\n",
    "\n",
    "    def update(self, X: DATA, y: LABELS) -> Self:\n",
    "        ...\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.iforest import IForest\n",
    "import odds_datasets\n",
    "\n",
    "\n",
    "for dataset in odds_datasets.datasets_names[1:]:\n",
    "    X, y = odds_datasets.load(dataset)\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    model = IForest().fit(X)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaves = np.array([est.apply(X) for est in model.detector_.estimators_])\n",
    "print(leaves.shape)\n",
    "\n",
    "common = [\n",
    "    [\n",
    "        np.array([len(np.unique(l)) for l in leaves[:, current_leaves == l]]).sum()\n",
    "        for l in np.unique(current_leaves)\n",
    "    ]\n",
    "    for current_leaves in leaves\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for c in common:\n",
    "    plt.hist(c,alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sum(common, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
